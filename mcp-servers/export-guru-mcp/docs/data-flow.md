# Export Guru MCP Data Flow

This document outlines the data flow patterns implemented in the Export Guru MCP, explaining how data moves between the frontend, MCP, connectors, and LLMs.

## Data Flow Patterns

The MCP implements several standard data flow patterns to handle different types of requests:

### 1. Direct Data Access Pattern

This pattern is used for simple data retrieval where no LLM processing is required.

```
Frontend -> MCP -> Connector -> Data Source -> Connector -> MCP -> Frontend
```

#### Example: Retrieving Trade Data

```typescript
// Frontend request
const response = await fetch('/api/trade-data?exporter=US&importer=EU&hsCode=8471');

// MCP processing (in trade-map.ts connector)
export async function getTradeData(
  exporterCountry: string,
  importerCountry: string,
  hsCode: string,
  year: number
): Promise<TradeData> {
  // Check cache
  const cacheKey = `trade-${exporterCountry}-${importerCountry}-${hsCode}-${year}`;
  const cachedData = cache.get(cacheKey);
  if (cachedData) {
    trackCacheHit(cacheKey, true);
    return cachedData;
  }
  
  // Fetch from API
  const data = await fetchFromTradeMapAPI(exporterCountry, importerCountry, hsCode, year);
  
  // Store in cache
  cache.set(cacheKey, data);
  
  return data;
}
```

### 2. LLM Enhancement Pattern

This pattern is used when data from a connector needs to be enhanced with LLM-generated insights.

```
Frontend -> MCP -> Connector -> Data Source -> Connector -> MCP -> LLM -> MCP -> Frontend
```

#### Example: Enhancing Regulatory Data

```typescript
// Frontend request
const response = await fetch('/api/regulatory-requirements?country=Germany&product=Electronics');

// MCP processing (in regulatory.ts)
export async function getEnhancedRegulatoryRequirements(
  country: string,
  productCategory: string,
  llm: LLM
): Promise<EnhancedRegulatoryRequirement[]> {
  // Get basic requirements from database
  const basicRequirements = await regulatoryDb.getRequirements(country, productCategory);
  
  // Enhance with LLM
  const enhancedRequirements = await enhanceRequirementsWithLLM(basicRequirements, llm);
  
  return enhancedRequirements;
}

async function enhanceRequirementsWithLLM(
  requirements: RegulatoryRequirement[],
  llm: LLM
): Promise<EnhancedRegulatoryRequirement[]> {
  // Create prompt
  const prompt = `Enhance the following regulatory requirements with additional details:
${JSON.stringify(requirements, null, 2)}`;
  
  // Get LLM response
  const response = await completeWithRetry(llm, prompt);
  
  // Parse response
  const enhancedRequirements = parseStructuredResponse<EnhancedRegulatoryRequirement[]>(response);
  
  return enhancedRequirements;
}
```

### 3. LLM Generation Pattern

This pattern is used when data is not available in connectors and needs to be generated by an LLM.

```
Frontend -> MCP -> LLM -> MCP -> Frontend
```

#### Example: Generating Market Entry Recommendations

```typescript
// Frontend request
const response = await fetch('/api/market-entry-recommendations', {
  method: 'POST',
  body: JSON.stringify({
    business: businessAnalysis,
    targetMarket: 'Japan'
  })
});

// MCP processing (in market-intelligence.ts)
export async function generateMarketEntryRecommendations(
  business: BusinessAnalysis,
  targetMarket: string,
  llm: LLM
): Promise<string[]> {
  // Create prompt
  const prompt = `Generate market entry recommendations for a ${business.categories[0].mainSector} business 
entering the ${targetMarket} market. The business has the following products:
${business.products.map(p => `- ${p.name}`).join('\n')}`;
  
  // Get LLM response
  const response = await completeWithRetry(llm, prompt);
  
  // Parse response
  const recommendations = parseStructuredResponse<{ recommendations: string[] }>(response);
  
  return recommendations.recommendations;
}
```

### 4. Hybrid Processing Pattern

This pattern is used for complex analyses requiring multiple data sources and LLM processing.

```
Frontend -> MCP -> [Multiple Connectors] -> MCP -> LLM -> MCP -> Frontend
```

#### Example: Creating a Comprehensive Market Report

```typescript
// Frontend request
const response = await fetch('/api/market-report', {
  method: 'POST',
  body: JSON.stringify({
    business: businessAnalysis,
    targetMarket: 'Germany'
  })
});

// MCP processing (in report.ts)
export async function createMarketReport(
  business: BusinessAnalysis,
  targetMarket: string,
  llm: LLM
): Promise<MarketReport> {
  // Get data from multiple sources
  const [
    marketInfo,
    regulatoryRequirements,
    tradeData,
    competitorAnalysis
  ] = await Promise.all([
    marketIntelligence.getMarketInfo(targetMarket),
    regulatory.getRegulatoryRequirements(targetMarket, business.categories[0].mainSector),
    tradeMap.getTradeData('Global', targetMarket, business.products[0].hsCode || '', new Date().getFullYear() - 1),
    marketIntelligence.getCompetitorAnalysis(targetMarket, business.categories[0].mainSector)
  ]);
  
  // Combine data
  const combinedData = {
    business,
    targetMarket,
    marketInfo,
    regulatoryRequirements,
    tradeData,
    competitorAnalysis
  };
  
  // Generate report with LLM
  const prompt = `Generate a comprehensive market report based on the following data:
${JSON.stringify(combinedData, null, 2)}`;
  
  // Get LLM response
  const response = await completeWithRetry(llm, prompt);
  
  // Parse response
  const report = parseStructuredResponse<MarketReport>(response);
  
  return report;
}
```

## Data Pipeline Implementation

The MCP implements these patterns using a data pipeline utility that standardizes the flow:

```typescript
// Example of using the data pipeline utility
export async function getEnhancedRegulatoryRequirements(
  country: string,
  productCategory: string,
  llm: LLM
): Promise<EnhancedRegulatoryRequirement[]> {
  const pipeline = createLLMEnhancementPipeline(
    // Data access function
    async () => await regulatoryDb.getRequirements(country, productCategory),
    
    // LLM enhancement function
    async (requirements) => {
      const prompt = `Enhance the following regulatory requirements with additional details:
${JSON.stringify(requirements, null, 2)}`;
      
      const response = await llm.complete({ prompt });
      return parseStructuredResponse<EnhancedRegulatoryRequirement[]>(response);
    },
    
    // LLM instance
    llm,
    
    // Options
    {
      cacheKey: `enhanced-regulatory-${country}-${productCategory}`,
      useCache: true,
      telemetry: true
    }
  );
  
  return await pipeline({});
}
```

## Error Handling in Data Flow

The data flow includes error handling at each stage:

1. **Connector Errors**: Handled with retries and fallbacks
2. **LLM Errors**: Handled with timeouts, retries, and fallbacks
3. **Parsing Errors**: Handled with validation and fallbacks
4. **Pipeline Errors**: Handled with telemetry and graceful degradation

## Caching in Data Flow

The data flow includes caching at multiple levels:

1. **Connector Caching**: Caches raw data from external sources
2. **LLM Caching**: Caches LLM responses for identical prompts
3. **Pipeline Caching**: Caches the results of entire pipelines

## Monitoring in Data Flow

The data flow includes monitoring at each stage:

1. **Response Time**: Tracks time taken for each stage
2. **Error Rate**: Tracks errors at each stage
3. **Cache Hit Rate**: Tracks cache effectiveness
4. **LLM Usage**: Tracks token usage for LLM calls

## Data Validation in Flow

The data flow includes validation at multiple points:

1. **Input Validation**: Validates input parameters
2. **Connector Validation**: Validates data from external sources
3. **LLM Response Validation**: Validates structured responses from LLMs
4. **Output Validation**: Validates final output before returning to frontend 